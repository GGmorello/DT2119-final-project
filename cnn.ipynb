{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 13:45:38.224541: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-17 13:45:39.178935: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-17 13:45:39.179087: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-17 13:45:39.179100: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 13:45:42.238246: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 13:45:42.251447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 13:45:42.253106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "# check available GPUs\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file data.npz\n",
    "dataset = np.load('data2.npz', allow_pickle=True)['data2'][()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bed': 0, 'cat': 1, 'dog': 2, 'five': 3, 'happy': 4, 'left': 5, 'marvin': 6, 'sheila': 7, 'six': 8, 'stop': 9}\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary to map the class names to integer labels\n",
    "label_dict = {key: v for v, key in enumerate(dataset.keys())}\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19873, 13, 44) (19873, 128, 128) (19873,)\n"
     ]
    }
   ],
   "source": [
    "data_dict = {\n",
    "    'lmfcc': [],\n",
    "    'mspec': [],\n",
    "    'label': []\n",
    "}\n",
    "for key in dataset.keys():\n",
    "    for utt in dataset[key]:\n",
    "        if utt['lmfcc'].shape[1] != 44:\n",
    "                    utt['lmfcc'] = np.pad(utt['lmfcc'], ((0,0),(0,44-utt['lmfcc'].shape[1])), 'constant', constant_values=0)\n",
    "        if utt['mspec'].shape[1] != 128:\n",
    "                    utt['mspec'] = np.pad(utt['mspec'], ((0,0),(0,128-utt['mspec'].shape[1])), 'constant', constant_values=0)\n",
    "        data_dict['lmfcc'].append(utt['lmfcc'])\n",
    "        data_dict['mspec'].append(utt['mspec'])\n",
    "        data_dict['label'].append(label_dict[key])\n",
    "\n",
    "# convert to numpy array\n",
    "data_dict['lmfcc'] = np.array(data_dict['lmfcc'])\n",
    "data_dict['mspec'] = np.array(data_dict['mspec'])\n",
    "data_dict['label'] = np.array(data_dict['label'])\n",
    "\n",
    "data = data_dict\n",
    "print(data['lmfcc'].shape, data['mspec'].shape, data['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand the dimension of lmfcc to (N, T, D, 1) for CNN\n",
    "data['lmfcc'] = np.expand_dims(data['lmfcc'], axis=-1)\n",
    "data['mspec'] = np.expand_dims(data['mspec'], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12718, 13, 44, 1) (12718,)\n",
      "(3180, 13, 44, 1) (3180,)\n",
      "(3975, 13, 44, 1) (3975,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# split the dataset into train, validation and test sets\n",
    "train_data, test_data, train_label, test_label = train_test_split(data['lmfcc'], data['label'], test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_label, val_label = train_test_split(train_data, train_label, test_size=0.2, random_state=42)\n",
    "\n",
    "print(train_data.shape, train_label.shape)\n",
    "print(val_data.shape, val_label.shape)\n",
    "print(test_data.shape, test_label.shape)\n",
    "\n",
    "# Convert labels to one-hot vectors\n",
    "train_label = to_categorical(train_label)\n",
    "val_label = to_categorical(val_label)\n",
    "test_label = to_categorical(test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "mfcc_height = data['lmfcc'].shape[1]\n",
    "mfcc_width = data['lmfcc'].shape[2]\n",
    "num_classes = len(dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers_list = [3, 4]\n",
    "# num_filters_list = [64, 128]\n",
    "# optimizer_list = [SGD(), Adam()]\n",
    "# num_nodes_list = [64, 128]\n",
    "# batch_size_list = [32, 64, 128]\n",
    "# regularization_list = [Dropout(0.2), Dropout(0.3), None, BatchNormalization()]\n",
    "\n",
    "# with open('results.txt', 'a') as file:\n",
    "#     for num_layers, num_filters, optimizer, num_nodes, batch_size, regularization in itertools.product(num_layers_list, num_filters_list, optimizer_list, num_nodes_list, batch_size_list, regularization_list):\n",
    "#         try:\n",
    "#             model = Sequential()\n",
    "\n",
    "#             model.add(Conv2D(num_filters, (3,3), activation='relu', input_shape=(mfcc_height, mfcc_width, 1)))\n",
    "\n",
    "#             # Add convolutional layers\n",
    "#             for _ in range(num_layers):\n",
    "#                 model.add(Conv2D(num_filters, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "#                 model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#                 if regularization is not None:\n",
    "#                     model.add(regularization)\n",
    "\n",
    "#             model.add(Flatten())\n",
    "\n",
    "#             # Add fully connected layers\n",
    "#             for _ in range(num_layers):\n",
    "#                 model.add(Dense(num_nodes, activation='relu'))\n",
    "#                 if regularization is not None:\n",
    "#                     model.add(regularization)\n",
    "\n",
    "#             model.add(Dense(10, activation='softmax'))  # Assuming a classification task with 10 classes\n",
    "\n",
    "#             # Compile the model\n",
    "#             model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "#             history = model.fit(train_data, train_label,\n",
    "#                                 validation_data=(val_data, val_label),\n",
    "#                                 epochs=20,\n",
    "#                                 batch_size=batch_size\n",
    "#                                 )\n",
    "            \n",
    "#             val_accuracy = history.history['val_accuracy'][-1]  # Get the validation accuracy from the last epoch\n",
    "\n",
    "#             print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "#             file.write(f\"Model: num_layers={num_layers}, num_filters={num_filters}, optimizer={optimizer.__class__.__name__}, num_nodes={num_nodes}, batch_size={batch_size}, regularization={regularization}\\n\")\n",
    "#             file.write(f\"Validation Accuracy: {val_accuracy}\\n\\n\")\n",
    "#         except:\n",
    "#             pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "199/199 [==============================] - 4s 11ms/step - loss: 0.5561 - acc: 0.8196 - val_loss: 0.4723 - val_acc: 0.8648\n",
      "Epoch 2/20\n",
      "199/199 [==============================] - 2s 10ms/step - loss: 0.1817 - acc: 0.9400 - val_loss: 0.2413 - val_acc: 0.9255\n",
      "Epoch 3/20\n",
      "199/199 [==============================] - 3s 13ms/step - loss: 0.1241 - acc: 0.9582 - val_loss: 0.2883 - val_acc: 0.9267\n",
      "Epoch 4/20\n",
      "199/199 [==============================] - 2s 12ms/step - loss: 0.0988 - acc: 0.9657 - val_loss: 0.2139 - val_acc: 0.9409\n",
      "Epoch 5/20\n",
      "199/199 [==============================] - 2s 10ms/step - loss: 0.0706 - acc: 0.9775 - val_loss: 0.2133 - val_acc: 0.9418\n",
      "Epoch 6/20\n",
      "199/199 [==============================] - 2s 10ms/step - loss: 0.0591 - acc: 0.9800 - val_loss: 0.2874 - val_acc: 0.9252\n",
      "Epoch 7/20\n",
      "199/199 [==============================] - 2s 10ms/step - loss: 0.0542 - acc: 0.9816 - val_loss: 0.1330 - val_acc: 0.9632\n",
      "Epoch 8/20\n",
      "199/199 [==============================] - 2s 10ms/step - loss: 0.0361 - acc: 0.9885 - val_loss: 0.1808 - val_acc: 0.9497\n",
      "Epoch 9/20\n",
      "199/199 [==============================] - 2s 11ms/step - loss: 0.0438 - acc: 0.9855 - val_loss: 0.2032 - val_acc: 0.9497\n",
      "Epoch 10/20\n",
      "199/199 [==============================] - 2s 10ms/step - loss: 0.0542 - acc: 0.9816 - val_loss: 0.2152 - val_acc: 0.9456\n",
      "Epoch 11/20\n",
      "199/199 [==============================] - 2s 10ms/step - loss: 0.0542 - acc: 0.9825 - val_loss: 0.2153 - val_acc: 0.9519\n",
      "Epoch 12/20\n",
      "199/199 [==============================] - 2s 10ms/step - loss: 0.0574 - acc: 0.9821 - val_loss: 0.2916 - val_acc: 0.9355\n",
      "Epoch 13/20\n",
      "199/199 [==============================] - 2s 9ms/step - loss: 0.0458 - acc: 0.9852 - val_loss: 0.1993 - val_acc: 0.9497\n",
      "Epoch 14/20\n",
      "199/199 [==============================] - 2s 9ms/step - loss: 0.0433 - acc: 0.9853 - val_loss: 0.1890 - val_acc: 0.9563\n",
      "Epoch 15/20\n",
      "199/199 [==============================] - 2s 10ms/step - loss: 0.0251 - acc: 0.9921 - val_loss: 0.1133 - val_acc: 0.9708\n",
      "Epoch 16/20\n",
      "199/199 [==============================] - 2s 10ms/step - loss: 0.0154 - acc: 0.9951 - val_loss: 0.1340 - val_acc: 0.9673\n",
      "Epoch 17/20\n",
      "199/199 [==============================] - 2s 9ms/step - loss: 0.0196 - acc: 0.9935 - val_loss: 0.1974 - val_acc: 0.9575\n",
      "Epoch 18/20\n",
      "199/199 [==============================] - 2s 10ms/step - loss: 0.0328 - acc: 0.9896 - val_loss: 0.2192 - val_acc: 0.9594\n",
      "Epoch 19/20\n",
      "199/199 [==============================] - 2s 10ms/step - loss: 0.0455 - acc: 0.9870 - val_loss: 0.2433 - val_acc: 0.9513\n",
      "Epoch 20/20\n",
      "199/199 [==============================] - 2s 9ms/step - loss: 0.0406 - acc: 0.9869 - val_loss: 0.2234 - val_acc: 0.9550\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(256, (3,3), activation='relu', input_shape=(mfcc_height, mfcc_width, 1)))\n",
    "model.add(MaxPooling2D((3,3), strides=(2,2), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(256, (3,3), activation='relu', input_shape=(mfcc_height, mfcc_width, 1)))\n",
    "model.add(MaxPooling2D((3,3), strides=(2,2), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(256, (2,2), activation='relu', input_shape=(mfcc_height, mfcc_width, 1)))\n",
    "model.add(MaxPooling2D((3,3), strides=(2,2), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(lr=0.001),\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics = ['acc']\n",
    "                     )\n",
    "history = model.fit(train_data, train_label,\n",
    "                        validation_data=(val_data, val_label),\n",
    "                        epochs=20,\n",
    "                        batch_size=64\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 2ms/step\n",
      "Test Accuracy: 0.9532\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# Convert the predictions to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predicted_labels == np.argmax(test_label, axis=1))\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
